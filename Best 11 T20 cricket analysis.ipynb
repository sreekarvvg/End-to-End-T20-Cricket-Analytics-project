{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cdede8f",
   "metadata": {},
   "source": [
    "The code imports several libraries: BeautifulSoup from bs4, requests, pandas, numpy, and json. These libraries are commonly used for web scraping, data manipulation, and working with JSON data.\n",
    "\n",
    "The BeautifulSoup library is used for parsing HTML and XML documents, requests is used for making HTTP requests to fetch web pages, pandas is a powerful data manipulation library, numpy provides support for numerical operations, and json is used for working with JSON data.\n",
    "\n",
    "By importing these libraries, the code sets up the necessary dependencies to perform tasks such as scraping web pages, processing data, and working with JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afc54ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d27116",
   "metadata": {},
   "source": [
    "### Step-1\n",
    "\n",
    "The code is fetching a match summary table from the ESPN Cricinfo page for the year 2022. It is using the BeautifulSoup library to parse the HTML of the web page and extract the desired table.\n",
    "\n",
    "Let's go through the code step by step:\n",
    "\n",
    "1. It starts by importing the necessary libraries: BeautifulSoup from bs4, requests, pandas, numpy, and json. These libraries are commonly used for web scraping, data manipulation, and working with JSON data.\n",
    "\n",
    "2. The code defines a variable URL which contains the web address of the page that needs to be scraped.\n",
    "\n",
    "3. The requests.get() function is used to send an HTTP GET request to the specified URL and retrieve the web page content. The response from the request is stored in the variable r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Collecting the match summary table for thee year 2022 from ESPN CRICK INFO page\n",
    "## This method is using Beautiful Soup. We are creating a soup and extracting the table\n",
    "URL = 'https://www.espncricinfo.com/records/year/team-match-results/2022-2022/twenty20-internationals-3'\n",
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text,'lxml')\n",
    "table = soup.find(class_=\"ds-overflow-x-auto ds-scrollbar-hide\")\n",
    "headers = table.find_all(\"span\", class_=\"ds-cursor-pointer\")\n",
    "body = table.find_all(\"td\", class_=\"ds-min-w-max\")\n",
    "titles = [i.text for i in headers]\n",
    "data = [j.text for j in body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Collecting the match summary table for thee year 2022 from ESPN CRICK INFO page\n",
    "## As we are collecting the table, we can also use pandas library to extract tables directly into dataframes\n",
    "URL = 'https://www.espncricinfo.com/records/year/team-match-results/2022-2022/twenty20-internationals-3'\n",
    "table = pd.read_html(URL)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71ef4b",
   "metadata": {},
   "source": [
    "#### The remaining data is extracted through Brighdata website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60241d9e",
   "metadata": {},
   "source": [
    "### Match result Summary\n",
    "\n",
    "1. The code is reading data from a JSON file and creating a pandas DataFrame to store the Match result Summary data.\n",
    "2. Cleaning and Transforming the Match result Summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t20_json_files/t20_wc_match_results.json') as f:\n",
    "    data = json.load(f)\n",
    "df_result = pd.DataFrame(data[0]['matchSummary'])\n",
    "df_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e35234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.rename(columns={'scorecard':'match_id'} ,inplace = True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f408b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('t20_csv_files/t20_wc_match_results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_id_dict = {}\n",
    "for index,row in df_result.iterrows():\n",
    "    key1 = row['team1']+ ' ' + 'Vs' + ' '+ row['team2']\n",
    "    key2 = row['team2']+ ' ' + 'Vs' + ' '+ row['team1']\n",
    "    match_id_dict[key1] = row['match_id']\n",
    "    match_id_dict[key2] = row['match_id']\n",
    "    \n",
    "match_id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46739c37",
   "metadata": {},
   "source": [
    "### Batting Summary\n",
    "\n",
    "1. The code is reading data from a JSON file and creating a pandas DataFrame to store the data.\n",
    "2. Cleaning and Transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t20_json_files/t20_wc_batting_summary.json') as f:\n",
    "    data = json.load(f)\n",
    "    bat_lst = []\n",
    "    for i in data:\n",
    "        bat_lst.extend(i['battingSummary']) \n",
    "df_bat = pd.DataFrame(bat_lst)\n",
    "df_bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bat['Result'] = df_bat.dismissal.apply(lambda x:'not out' if x==\"\" else \"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bat.drop(columns = 'dismissal' ,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bat['batsmanName'] = df_bat['batsmanName'].apply(lambda x:x.replace('â€',''))\n",
    "df_bat.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67bd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bat['match_id'] = df_bat['match'].map(match_id_dict)\n",
    "df_bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d19aec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bat.to_csv('t20_csv_files/t20_wc_batting_summary.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc14c7",
   "metadata": {},
   "source": [
    "### Bowling Summary\n",
    "\n",
    "1. The code is reading data from a JSON file and creating a pandas DataFrame to store the data.\n",
    "2. Cleaning and Transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t20_json_files/t20_wc_bowling_summary.json') as f:\n",
    "    data = json.load(f)\n",
    "    all_rec = []\n",
    "    for rec in data:\n",
    "        all_rec.extend(rec['bowlingSummary'])\n",
    "df_bowl = pd.DataFrame(all_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bowl['match_id'] = df_bowl['match'].map(match_id_dict)\n",
    "df_bowl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "060421ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bowl.to_csv('t20_csv_files/t20_wc_bowling_summary.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8723b7f",
   "metadata": {},
   "source": [
    "### Player Info\n",
    "\n",
    "1. The code is reading data from a JSON file and creating a pandas DataFrame to store the data.\n",
    "2. Cleaning and Transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t20_json_files/t20_wc_player_info.json') as f:\n",
    "    data = json.load(f)\n",
    "df_info = pd.DataFrame(data)\n",
    "df_info['name'] = df_info['name'].apply(lambda x:x.replace('â€',''))\n",
    "df_info.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dafde972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info.to_csv('t20_csv_files/t20_wc_player_info.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
